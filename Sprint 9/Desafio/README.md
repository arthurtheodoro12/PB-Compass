<h1 align="center">Resolu√ß√£o do Desafio ‚úçÔ∏è</h1>

### üìù Explica√ß√£o sobre o desafio

O objetivo deste desafio √© desenvolver a camada Refined do data lake do desafio final, aplicando uma modelagem dimensional aos dados que anteriormente estavam armazenados na camada Trusted. Tudo isso, utilizado o Apache Spark SQL por meio do servi√ßo AWS Glue, e o Glue Crawler para criar as tabelas dimensionais e a tabela fato, permitindo que os dados sejam consultados posteriormente no Amazon QuickSight.  

### Minhas Perguntas para o desafio final

- Nessa etapa, precisei ajustar duas das perguntas que havia planejado inicialmente. Ao analisar melhor a base de dados, percebi que n√£o haveria informa√ß√µes suficientes no CSV sobre os atores dos filmes de guerra. Por isso, adaptei as perguntas para focar apenas nos filmes, deixando de lado os detalhes sobre atores e diretores.

#### Novas Perguntas:

1. Como a frequ√™ncia de lan√ßamentos de filmes de crime/guerra variou ao longo das d√©cadas da Guerra Fria (1947-1991)?
 
2. Quais s√£o os temas ou enredos mais comuns nos filmes de crime/guerra durante os momentos de maior tens√£o da Guerra Fria, como a Crise dos M√≠sseis de Cuba (1962) ou a Guerra do Vietn√£ (1955-1975)?
 
3. Como o pa√≠s de produ√ß√£o dos filmes influenciou a representa√ß√£o de nacionalidades e conflitos nas narrativas cinematogr√°ficas?
 
4. Existe uma tend√™ncia nas produ√ß√µes de filmes de crime/guerra ao longo da Guerra Fria que reflete mudan√ßas nas percep√ß√µes p√∫blicas de amea√ßas internacionais?
 
5. Como a popularidade m√©dia e a avalia√ß√£o do p√∫blico dos filmes de crime/guerra variaram ao longo do per√≠odo da Guerra Fria, e h√° correla√ß√£o com eventos hist√≥ricos espec√≠ficos?"

### 1¬∫ Passo: An√°lise Dimensional

- Diagrama Dimensional:
- ![desenho do driagrama dimensional Sprint09](../Evidencias/DiagramaDimensional.png)

#### Tabela Fato
- A tabela fato armazena as principais m√©tricas relacionadas ao desempenho dos filmes, como as colunas ``popularidade``, ``nota_media`` e ``total_votos``. Al√©m disso, ela cont√©m todas as chaves estrangeiras das demais tabelas, que s√£o: ``id_filme``, ``id_genero``, ``id_idioma`` e ``id_tempo``.

#### Dimens√µes
- Optei por criar quatro dimens√µes e uma tabela de liga√ß√£o para resolver a rela√ß√£o de muitos para muitos (N:N) entre as tabelas ``dim_genero`` e ``dim_filme``. Essa abordagem foi necess√°ria pois um filme pode pertencer a v√°rios g√™neros, e um g√™nero pode estar associado a v√°rios filmes. A tabela de liga√ß√£o serve justamente para que n√£o haja problemas nessa rela√ß√£o, garantindo a normaliza√ß√£o das dimens√µes.

- Dimens√£o Idioma: Possui todos os idiomas presente nos filmes, al√©m de armazenar o id de cada idioma

- Dimens√£o Tempo: Possui todas as datas de lan√ßamentos dos filmes, al√©m de armazenar o id de cada data

- Dimens√£o Filme: Possui todos os dados que n√£o fazem respeito ao desempenho dos filmes, como ``titulo``, ``titulo_original`` e ``sinopse``.

- Dim_Genero: Possui todos os g√™neros dos filmes, nominalmente, al√©m de armazenar o id de cada g√™nero

- Filme_Genero: Tabela de liga√ß√£o, respons√°vel por manter a normaliza√ß√£o das tabelas ``dim_filme`` e ``dim_genero``

### 2¬∫ Passo: Script

- Com as dimens√µes j√° definidas, a cria√ß√£o do script se tornou muito mais f√°cil. Bastou transformar o [c√≥digo respons√°vel por criar as dimens√µes](../Desafio/Dimensional.sql) em Spark SQL, o adaptando para ser utilizado no AWS Glue e para ler e gravar dados no S3.

#### O Script
- O [script](../Desafio/Dimensional.py) criado processa os dados da Trusted, os transforma em tabelas dimensionais, de liga√ß√£o e uma tabela fato, e ent√£o salva o resultado na camada refined.

#### Importa√ß√£o das bibliotecas e inicializa√ß√£o do Spark e GlueContext

- O primeiro passo do Script foi importar as bibliotecas e as fun√ß√µes do PySpark que ser√£o utilizadas no c√≥digo futuramente.
    - `````````
        import sys
        from pyspark.context import SparkContext
        from pyspark.sql.functions import col, explode, monotonically_increasing_id, when
        from awsglue.transforms import *
        from awsglue.utils import getResolvedOptions
        from awsglue.context import GlueContext
        ``````````

###

- Ap√≥s a importa√ß√£o das bibliotecas, foi poss√≠vel iniciar o Spark e o GlueContext 
    - ````````
        args = getResolvedOptions(sys.argv, ["JOB_NAME"])
        sparkContext = SparkContext()
        glueContext = GlueContext(sparkContext)
        spark = glueContext.spark_session
        ````````

#### Cria√ß√£o do dataframe

- Com o Spark e o GlueContext devidamente iniciados, foi poss√≠vel ler os dados persistidos na camada Trusted do S3, atrav√©s do ``dynamic_frame`` do GlueContext e posteriormente transforma-los em um dataframe do Spark
    - ```````
        caminho_trusted = "s3://data-lake-arthur-theodoro/Trusted/TMDB/JSON/25/02/03/"
        df_trusted = glueContext.create_dynamic_frame.from_options(
            format="parquet",
            connection_type="s3",
            connection_options={"paths": [caminho_trusted]},
            transformation_ctx="df_trusted"
        ).toDF()
        ````````

#### Cria√ß√£o das Dimens√µes/Tabela de Liga√ß√£o/Tabela Fato

- Agora que todas as etapas iniciais foram feitas, ent√£o comecei a efetivamente criar as dimens√µes.

#### dim_filme
 
- A primeira dimens√£o criada foi a ``dim_filme``. Como todas as colunas necess√°rias j√° estavam presentes nos dados persistidos na camada trusted, bastou selecionar as colunas usando um ``select()`` e renomear as colunas com o m√©todo ``alias()``.
    - `````
        dim_filme = df_trusted.select(
        col("id").alias("id_filme"),
        col("title").alias("titulo"),
        col("original_title").alias("titulo_original"),
        col("overview").alias("sinopse")
        ).distinct()
        ``````
- Para gravar a tabela na camada refined, utilizei o m√©todo ``write.mode(overwirte)``, para sobrescrever qualquer arquivo existente no caminho especificado do S3. O caminho definido foi o da camada refined, dentro da pasta ``dim_filme``, e o formato do arquivo foi definido como parquet.
    - ```````
        dim_filme.write.mode("overwrite").parquet("s3://data-lake-arthur-theodoro/Refined/dim_filme/")

         ````````

#### dim_genero
- Na codifica√ß√£o da ``dim_genero``, precisei utilizar o m√©todo ``explode()``, na cria√ß√£o da coluna ``id_genero``, para garantir que cada g√™nero fosse separado em uma linha √∫nica. Isso foi necess√°rio porque os g√™neros estavam originalmente agrupados em uma √∫nica coluna. Combinando o ``explode()`` com o m√©todo ``distinct()``, consegui garantir que cada g√™nero fosse armazenado apenas uma vez, atribuindo um ``id_genero`` √∫nico para cada um deles. 
    - ````````
        dim_genero = df_trusted.select(explode(col("genre_ids")).alias("id_genero")).distinct()
        ````````
- Para a coluna ``nome_genero``, utilizei a fun√ß√£o ``when()`` para associar corretamente cada nome de g√™nero ao seu respectivo ``id_genero``.
    - ```````
        dim_genero = dim_genero.withColumn(
        "nome_genero",
        when(col("id_genero") == 10752, "Guerra")
        .when(col("id_genero") == 28, "A√ß√£o")
        .when(col("id_genero") == 12, "Aventura")
        .when(col("id_genero") == 878, "Fic√ß√£o Cient√≠fica")
        .when(col("id_genero") == 10749, "Romance")
        .when(col("id_genero") == 36, "Hist√≥ria")
        .when(col("id_genero") == 99, "Document√°rio")
        .when(col("id_genero") == 10402, "Musical")
        .when(col("id_genero") == 80, "Crime")
        .when(col("id_genero") == 18, "Drama")
        .when(col("id_genero") == 10770, "Filme de TV")
        .when(col("id_genero") == 27, "Terror")
        .when(col("id_genero") == 35, "Com√©dia")
        .when(col("id_genero") == 16, "Anima√ß√£o")
        .when(col("id_genero") == 10751, "Familia")
        .when(col("id_genero") == 14, "Fantasia")
        .when(col("id_genero") == 53, "Suspense")
        .when(col("id_genero") == 37, "Velho Oeste")
        .when(col("id_genero") == 9648, "Mist√©rio")
        .otherwise("Desconhecido")
        ````````

- Para gravar a tabela no S3, segui a mesma l√≥gica da ``dim_filme``
    - ````````
        dim_genero.write.mode("overwrite").parquet("s3://data-lake-arthur-theodoro/Refined/dim_genero/")
        ````````

#### dim_idioma e dim_tempo

- Para as dimens√µes ``dim_idioma`` e ``dim_tempo``, segui a mesma l√≥gica na cria√ß√£o. Primeiro, selecionei as colunas j√° existentes nos dados persistidos na camada trusted. Em seguida, utilizei o m√©todo ``monotonically_increasing_id()`` para gerar os IDs √∫nicos de ambas as tabelas. Para salvar as dimens√µes no S3, utilizei a mesma l√≥gica da ``dim_filme``
    - ``````
        #dim_idioma
        dim_idioma = df_trusted.select(col("original_language").alias("codigo_idioma")).distinct().withColumn("id_idioma", monotonically_increasing_id())

        dim_idioma.write.mode("overwrite").parquet("s3://data-lake-arthur-theodoro/Refined/dim_idioma/")


        #dim_Tempo
        dim_tempo = df_trusted.select(col("release_date").alias("data_lancamento")).distinct().withColumn("id_tempo", monotonically_increasing_id())
    
        dim_tempo.write.mode("overwrite").parquet("s3://data-lake-arthur-theodoro/Refined/dim_tempo/")
        ````````

#### filme_genero
- Na tabela de liga√ß√£o ``filme_genero`` utilizei o select para selecionar a coluna ``id_filme`` que j√° existia nos dados persistidos no S3 e utilizei o m√©todo ``explode()``, seguindo a mesma l√≥gica de quando ele foi utilizado na ``dim_genero``. Para salvar a dimens√£o no S3, utilizei a mesma l√≥gica da ``dim_filme``
    - ````````
        #filme_genero (Tabela de liga√ß√£o)
        filme_genero = df_trusted.select(col("id").alias("id_filme"),explode(col("genre_ids")).alias("id_genero"))

        filme_genero.write.mode("overwrite").parquet("s3://data-lake-arthur-theodoro/Refined/filme_genero/")
        ``````````

#### fato_filme
- Ap√≥s a cria√ß√£o de todas as dimens√µes, foi poss√≠vel criar a tabela .
- Na cria√ß√£o da tabela fato, foi necess√°rio fazer 2 Joins, o primeiro foi dos dados persistidos na trusted com a dim_idioma e o segundo com a dim_tempo, ambos vizando conseguir pegar o id das dimens√µes. 
- Ap√≥s os Joins, ent√£o bastou selecionar as colunas que a tabela fato iria armazenar, e assim foi feito.
- Para salvar a dimens√£o no S3, utilizei a mesma l√≥gica da ``dim_filme``
    - ````````
        fato_filme = df_trusted.join(dim_idioma, df_trusted.original_language == dim_idioma.codigo_idioma).join(dim_tempo, df_trusted.release_date == dim_tempo.data_lancamento) \
        .select(
        monotonically_increasing_id().alias("id_fato_filme"),
        col("id").alias("id_filme"),
        explode(col("genre_ids")).alias("id_genero"),
        col("id_idioma"),
        col("id_tempo"),
        col("popularity").alias("popularidade"),
        col("vote_average").alias("nota_media"),
        col("vote_count").alias("total_votos")
        )
                       
        fato_filme.write.mode("overwrite").parquet("s3://data-lake-arthur-theodoro/Refined/fato_filme/")
        ````````

### 3¬∫ Passo: Execu√ß√£o do Job Glue

- Ap√≥s a cria√ß√£o do script e a sua execu√ß√£o, os dados foram persistidos corretamente no S3.

- Evid√™ncia dos dados no S3
![pastas dimensoes no S3](../Evidencias/CriacaoPastarS3.png)

### 4¬∫ Passo: Execu√ß√£o do Crawler e cria√ß√£o das tabelas

- Com os dados corretamente armazenados no S3, foi poss√≠vel configurar um crawler no AWS Glue. Esse crawler varreu todos os arquivos da camada Refined e criou automaticamente as tabelas no Glue Data Catalog. Com as tabelas dispon√≠veis no GlueContext, elas agora est√£o prontas para serem utilizadas no Amazon QuickSight.

- Evid√™ncia da cria√ß√£o da Refined Crawler e sua correta execu√ß√£o
- ![refined crawler](../Evidencias/CriacaoRefinedCrawler.png)

- Evid√™ncias das cria√ß√£o correta das tabelas no Glue Data Catalog
- ![tabelas 1](../Evidencias/CriacaoTabelas1.png)
- ![tabelas 2](../Evidencias/CriacaoTabelas2.png)
- ![tabelas 3](../Evidencias/CriacaoTabelas3.png)