<h1 align="center">Resolu√ß√£o do Desafio ‚úçÔ∏è</h1>

### üìù Explica√ß√£o sobre o desafio

O objetivo deste desafio foi utilizar o Apache Spark por meio do servi√ßo AWS Glue para integrar dados da camada RAW Zone do Data Lake do desafio final √† Trusted Zone. Para isso, os dados foram tratados, transformados em formato Parquet e, em seguida, inseridos na Trusted Zone.

### Reingest√£o dos dados na camada RAW

O [desafio da sprint passada](/Sprint%207/Desafio/README.md) foi justamente ingerir dados na camada RAW do Data Lake, e isso foi feito com sucesso. No entanto, percebi que, para responder de maneira satisfat√≥ria √†s perguntas da minha an√°lise, seria necess√°rio obter mais dados da API do TMDB. Por isso, coletei todos os filmes dentro dos filtros previamente definidos, totalizando 4.800 t√≠tulos, uma amostra satisfat√≥ria.

- Novos dados da camada RAW no S3
- ![Novos dados da camada RAW no S3](../Evidencias/ReingestaoDados.png)

### Processamento CSV

- O c√≥digo [ProcessamenoCSV.py](./ProcessamentoCSV.py) √© o respons√°vel por tratar, transformar em parquet e upar para o S3 o arquivo movies.sv persistido na RAW Zone, tudo isso atrav√©s do AWS Glue

- O primeiro passo do c√≥digo foi importar todas as bibliotecas que ser√£o usadas nele.
    - ```````
        import sys
        from pyspark.sql import SparkSession
        from awsglue.context import GlueContext
        from awsglue.job import Job
        from awsglue.utils import getResolvedOptions
        from awsglue.dynamicframe import DynamicFrame
        from pyspark.sql.functions import col, current_date
        `````````

- Ap√≥s a importa√ß√£o das bibliotecas, foi poss√≠vel iniciar o desenvolvimento do c√≥digo. Como a execu√ß√£o ocorre no AWS Glue, foi necess√°rio definir os par√¢metros do JOB para garantir a correta interpreta√ß√£o e funcionamento dentro do servi√ßo.

    - ```````
        args = getResolvedOptions(sys.argv, ['JOB_NAME'])
        ````````

- Com os argumentos definidos, foi necess√°rio iniciar a sess√£o Spark para possibilitar o uso do Spark no c√≥digo. Al√©m disso, tamb√©m foi iniciada a GlueContext, que estende a funcionalidade do Spark, permitindo a integra√ß√£o com os servi√ßos do AWS Glue e facilitando o processamento e a transforma√ß√£o dos dados.

    - ````````
        spark = SparkSession.builder.appName(args['JOB_NAME']). getOrCreate()
        glueContext = GlueContext(spark.sparkContext)
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)

        ``````````
- Com as configura√ß√µes iniciais feitas, ent√£o defini os caminhos. Tanto o caminho no qual o arquivo que ser√° tratado no c√≥digo est√°, quanto o caminho que o arquivo, ao final de todo o processamento, ser√° persistido no S3.
    - ````````
        raw_path = 's3://data-lake-arthur-theodoro/Raw/Local/CSV/Movies/2025/01/06/movies.csv'
        trusted_path = 's3://data-lake-arthur-theodoro/Trusted/Local/CSV/movie.parquet'
        ``````````
- Uma vez que os caminhos foram definidos, se tornou poss√≠vel criar um DataFrame, ent√£o fiz isso utilizando o Spark
    - ````````
        df = spark.read.option("header", "true").csv(raw_path)
        ````````

- A cria√ß√£o do DataFrame marcou o in√≠cio do tratamento dos dados. No caso do arquivo CSV, optei por lidar apenas com os valores nulos, uma vez que todas as colunas ser√£o utilizadas futuramente. Assim, decidi deixar o tratamento mais detalhado para a etapa de normaliza√ß√£o do banco de dados.

    - `````````
        df = df.fillna(0)
        ``````````

- Agora, com todas as etapas anteriores conclu√≠das, foi poss√≠vel transformar o DataFrame originado do CSV para o formato Parquet e envi√°-lo para o S3 na Trusted Zone. O m√©todo mais simples que encontrei foi converter o DataFrame em um DynamicFrame, o que facilita o AWS Glue a processar os dados, e utilizar o m√©todo ``write_dynamic_frame.from_options()`` para converter o arquivo para Parquet e envi√°-lo ao S3.
    - ````````
        # Convertendo para DynamicFrame
        dynamic_frame = DynamicFrame.fromDF(df, glueContext, "dynamic_frame")
    
        #Mudando para Parquet e upando para o S3
        glueContext.write_dynamic_frame.from_options(
            frame=dynamic_frame,
            connection_type="s3",
            connection_options={"path": trusted_path},
            format="parquet"
            )
        ````````

### Processamento JSON

- O arquivo [ProcessamentoJSON.py](../Desafio/ProcessamentoJSON.py) segue a mesma estrutura do [ProcessamenoCSV.py](./ProcessamentoCSV.py), as maiores diferen√ßas est√£o no caminho de sa√≠da dos JSON e no tratamento de dados dos mesmos.

#### Caminho de sa√≠da
- O caminho de sa√≠da do JSON precisava, obrigatoriamente, conter a data do dia em que os arquivos foram enviados para o S3, a fim de seguir corretamente a estrutura de pastas esperada na Trusted Zone.
- Para garantir essa estrutura, utilizei os m√©todos select() e collect() do PySpark para obter a data desejada. Em seguida, concatenei essa informa√ß√£o ao caminho de sa√≠da base, de forma semelhante ao que foi feito para o arquivo CSV.
    - ```````
        data_processamento = df.select(date_format(current_date(), "yy/MM/dd")).collect()[0][0]

        # Defininido o caminho de sa√≠da
        path_saida = f"{path_de_base}{data_processamento}/"
        ``````````

#### Tratamento dos Dados
- Diferentemente do CSV, nem todas as colunas do arquivo JSON ser√£o utilizadas. Por isso, aproveitei essa etapa de tratamento para filtrar apenas as colunas relevantes para a an√°lise do desafio final, mantendo, no entanto, o tratamento mais detalhado para a etapa de normaliza√ß√£o das tabelas.
    - `````````
        #Selecionando colunas relevantes
        df = df.select("genre_ids", "id", "original_language", "original_title", "overview", "popularity", "release_date", "title", "vote_average", "vote_count")

        #Tratamento de valores nulos
        df = df.fillna(0)       
        ``````````

### Execu√ß√£o dos Jobs
- Com ambos os Jobs Criados, ent√£o parti para a execu√ß√£o deles.
    - Comprova√ß√£o da execu√ß√£o bem sucedida de ambos os Jobs:
    ![Comprova√ß√£o execu√ß√£o](../Evidencias/JobsRodadosSucesso.png)
    - Comprova√ß√£o da Trusted Zone do CSV
    ![Comprova√ß√£o Trusted CSV](../Evidencias/TrustedCSV.png)
    - Comprova√ß√£o da Trusted Zone do JSON
    ![Comprova√ß√£o Trusted JSON](../Evidencias/TrustedJSON.png)